{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T01:52:20.770804Z",
     "start_time": "2025-04-04T01:52:14.834141Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pdfplumber  # Using pdfplumber instead of PyMuPDF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel\n",
    "import warnings\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Extract text from PDF files using pdfplumber\"\"\"\n",
    "\n",
    "    def __init__(self, directory_path):\n",
    "        \"\"\"\n",
    "        Initialize with the directory containing PDF files\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        directory_path : str\n",
    "            Path to directory containing PDF files\n",
    "        \"\"\"\n",
    "        self.directory_path = directory_path\n",
    "        self.pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "        self.documents = {}\n",
    "\n",
    "    def extract_all_texts(self):\n",
    "        \"\"\"\n",
    "        Extract text from all PDF files in the directory\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary with filename as key and extracted text as value\n",
    "        \"\"\"\n",
    "        for pdf_file in self.pdf_files:\n",
    "            file_path = os.path.join(self.directory_path, pdf_file)\n",
    "            self.documents[pdf_file] = self.extract_text(file_path)\n",
    "        return self.documents\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_text(file_path):\n",
    "        \"\"\"\n",
    "        Extract text from a single PDF file using pdfplumber\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : str\n",
    "            Path to PDF file\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str : Extracted text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    extracted = page.extract_text()\n",
    "                    if extracted:\n",
    "                        text += extracted + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Preprocess text data\"\"\"\n",
    "\n",
    "    def __init__(self, additional_stopwords=None):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with optional additional stopwords\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        additional_stopwords : list, optional\n",
    "            List of additional stopwords to remove\n",
    "        \"\"\"\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if additional_stopwords:\n",
    "            self.stop_words.update(additional_stopwords)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess a text document\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Raw text to preprocess\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str : Preprocessed text\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove non-alphabetic characters and extra whitespace\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and short tokens, lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if\n",
    "                  token not in self.stop_words and len(token) > 2]\n",
    "\n",
    "        # Join back into a string\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def preprocess_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Preprocess a dictionary of documents\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        documents : dict\n",
    "            Dictionary with document names as keys and raw text as values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary with document names as keys and preprocessed text as values\n",
    "        \"\"\"\n",
    "        return {doc_name: self.preprocess(text) for doc_name, text in documents.items()}\n",
    "\n",
    "\n",
    "class DocumentClustering:\n",
    "    \"\"\"Cluster documents into groups\"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=2):\n",
    "        \"\"\"\n",
    "        Initialize with the number of clusters\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_clusters : int, optional\n",
    "            Number of clusters to create (default is 2 for Accounting and Finance)\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.85)\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.document_names = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.pca = PCA(n_components=2, random_state=42)\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, preprocessed_documents):\n",
    "        \"\"\"\n",
    "        Fit the clustering model to preprocessed documents\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        preprocessed_documents : dict\n",
    "            Dictionary with document names as keys and preprocessed text as values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        self.document_names = list(preprocessed_documents.keys())\n",
    "        texts = [preprocessed_documents[doc] for doc in self.document_names]\n",
    "\n",
    "        # Create TF-IDF matrix\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Apply KMeans clustering\n",
    "        self.labels = self.kmeans.fit_predict(self.tfidf_matrix)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_cluster_terms(self, n_terms=20):\n",
    "        \"\"\"\n",
    "        Get top terms for each cluster\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_terms : int, optional\n",
    "            Number of top terms to return\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary with cluster IDs as keys and lists of top terms as values\n",
    "        \"\"\"\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        cluster_terms = {}\n",
    "\n",
    "        # Calculate cluster centroids\n",
    "        order_centroids = self.kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "        # Extract top terms for each cluster\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            top_term_indices = order_centroids[cluster_id, :n_terms]\n",
    "            top_terms = [feature_names[i] for i in top_term_indices]\n",
    "            cluster_terms[cluster_id] = top_terms\n",
    "\n",
    "        return cluster_terms\n",
    "\n",
    "    def identify_outliers(self, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Identify potential outlier documents that don't fit well in any cluster\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        threshold : float, optional\n",
    "            Distance threshold for identifying outliers\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list : List of potential outlier document names\n",
    "        \"\"\"\n",
    "        # Calculate distances to assigned cluster center\n",
    "        distances = np.min(\n",
    "            [np.linalg.norm(self.tfidf_matrix.toarray() - center, axis=1)\n",
    "             for center in self.kmeans.cluster_centers_],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Identify outliers\n",
    "        outlier_indices = np.where(distances > threshold)[0]\n",
    "        outliers = [self.document_names[i] for i in outlier_indices]\n",
    "\n",
    "        return outliers\n",
    "\n",
    "    def get_cluster_documents(self):\n",
    "        \"\"\"\n",
    "        Get documents assigned to each cluster\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary with cluster IDs as keys and lists of document names as values\n",
    "        \"\"\"\n",
    "        cluster_docs = {}\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            indices = np.where(self.labels == cluster_id)[0]\n",
    "            cluster_docs[cluster_id] = [self.document_names[i] for i in indices]\n",
    "\n",
    "        return cluster_docs\n",
    "\n",
    "    def get_representative_documents(self):\n",
    "        \"\"\"\n",
    "        Get the most representative document for each cluster (closest to centroid)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary with cluster IDs as keys and representative document names as values\n",
    "        \"\"\"\n",
    "        representative_docs = {}\n",
    "\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            # Get indices of documents in this cluster\n",
    "            cluster_indices = np.where(self.labels == cluster_id)[0]\n",
    "\n",
    "            # Get documents in this cluster\n",
    "            cluster_docs = [self.document_names[i] for i in cluster_indices]\n",
    "\n",
    "            if len(cluster_docs) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate distance to centroid for each document in cluster\n",
    "            centroid = self.kmeans.cluster_centers_[cluster_id]\n",
    "            cluster_vectors = self.tfidf_matrix[cluster_indices].toarray()\n",
    "\n",
    "            # Find document closest to centroid\n",
    "            distances = np.linalg.norm(cluster_vectors - centroid, axis=1)\n",
    "            closest_idx = np.argmin(distances)\n",
    "\n",
    "            # Get the document name\n",
    "            representative_docs[cluster_id] = cluster_docs[closest_idx]\n",
    "\n",
    "        return representative_docs\n",
    "\n",
    "\n",
    "class SimilarityClassifier:\n",
    "    \"\"\"Classify documents based on similarity to cluster centers\"\"\"\n",
    "\n",
    "    def __init__(self, vectorizer, kmeans_model, cluster_mapping=None):\n",
    "        \"\"\"\n",
    "        Initialize with the vectorizer and KMeans model\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        vectorizer : TfidfVectorizer\n",
    "            Fitted vectorizer\n",
    "        kmeans_model : KMeans\n",
    "            Fitted KMeans model\n",
    "        cluster_mapping : dict, optional\n",
    "            Mapping from cluster IDs to subject names\n",
    "        \"\"\"\n",
    "        self.vectorizer = vectorizer\n",
    "        self.kmeans_model = kmeans_model\n",
    "        self.cluster_mapping = cluster_mapping or {}\n",
    "        self.threshold = 0.2  # Similarity threshold for classification\n",
    "\n",
    "    def classify(self, text, preprocess_func=None):\n",
    "        \"\"\"\n",
    "        Classify a new document\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to classify\n",
    "        preprocess_func : function, optional\n",
    "            Function to preprocess the text\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (predicted_class, confidence_score, is_outlier)\n",
    "        \"\"\"\n",
    "        if preprocess_func:\n",
    "            text = preprocess_func(text)\n",
    "\n",
    "        # Transform text to TF-IDF\n",
    "        text_tfidf = self.vectorizer.transform([text])\n",
    "\n",
    "        # Calculate distances to cluster centers\n",
    "        distances = [np.linalg.norm(text_tfidf.toarray() - center)\n",
    "                     for center in self.kmeans_model.cluster_centers_]\n",
    "\n",
    "        # Find closest cluster\n",
    "        closest_cluster = np.argmin(distances)\n",
    "        min_distance = distances[closest_cluster]\n",
    "\n",
    "        # Check if outlier\n",
    "        is_outlier = min_distance > self.threshold\n",
    "\n",
    "        # Map cluster to subject if mapping exists\n",
    "        predicted_class = self.cluster_mapping.get(closest_cluster, f\"Cluster {closest_cluster}\")\n",
    "\n",
    "        # Calculate confidence as inverse of normalized distance\n",
    "        max_distance = max(distances)\n",
    "        if max_distance > 0:\n",
    "            confidence = 1 - (min_distance / max_distance)\n",
    "        else:\n",
    "            confidence = 1.0\n",
    "\n",
    "        return predicted_class, confidence, is_outlier\n",
    "\n",
    "\n",
    "class KeywordClassifier:\n",
    "    \"\"\"Classify documents based on accounting and finance keywords\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with accounting and finance keywords\"\"\"\n",
    "        # Accounting keywords based on domain knowledge\n",
    "        self.accounting_keywords = [\n",
    "            'accounting', 'principles', 'standards', 'financial reporting', 'accounting standards',\n",
    "            'financial statement', 'balance sheet', 'income statement', 'cash flow',\n",
    "            'accounting', 'audit', 'ledger', 'journal entry', 'debit', 'credit',\n",
    "            'accounts payable', 'accounts receivable', 'asset', 'liability', 'equity',\n",
    "            'taxation', 'financial reporting', 'bookkeeping', 'accrual', 'depreciation',\n",
    "            'amortization', 'inventory', 'cost accounting', 'budgeting', 'variance analysis',\n",
    "            'profit', 'loss', 'revenue recognition', 'internal control', 'ifrs', 'gaap'\n",
    "        ]\n",
    "\n",
    "        # Finance keywords based on domain knowledge\n",
    "        self.finance_keywords = [\n",
    "            'investment', 'portfolio', 'risk', 'return', 'capital', 'valuation',\n",
    "            'interest rate', 'bond', 'stock', 'market', 'security', 'option', 'futures',\n",
    "            'derivative', 'dividend', 'corporate finance', 'capm', 'present value',\n",
    "            'npv', 'irr', 'wacc', 'capital structure', 'leverage', 'beta', 'alpha',\n",
    "            'financial market', 'efficient market', 'arbitrage', 'hedging', 'diversification',\n",
    "            'financial management', 'merger', 'acquisition'\n",
    "        ]\n",
    "\n",
    "        # Syllabus keywords\n",
    "        self.syllabus_keywords = [\n",
    "            'syllabus', 'course outline', 'learning objective', 'prerequisite',\n",
    "            'textbook', 'required reading', 'grading', 'assessment', 'assignment',\n",
    "            'lecture', 'class schedule', 'course description', 'instructor', 'professor',\n",
    "            'office hours', 'academic integrity', 'plagiarism', 'course policy',\n",
    "            'attendance', 'participation', 'final exam', 'midterm'\n",
    "        ]\n",
    "\n",
    "    def is_syllabus(self, text):\n",
    "        \"\"\"\n",
    "        Check if the document is likely a syllabus\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to analyze\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        bool : True if document is likely a syllabus, False otherwise\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return False\n",
    "\n",
    "        text_lower = text.lower()\n",
    "        # Count syllabus keywords\n",
    "        syllabus_count = sum(text_lower.count(keyword) for keyword in self.syllabus_keywords)\n",
    "\n",
    "        # If document contains at least 5 syllabus keywords, consider it a syllabus\n",
    "        return syllabus_count >= 5\n",
    "\n",
    "    def classify_by_filename(self, filename):\n",
    "        \"\"\"\n",
    "        Classify document based on filename\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            Name of the file to classify\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (predicted_class, confidence_score)\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            return \"Unknown\", 0.0\n",
    "\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        accounting_indicators = ['acc', 'acct', 'accounting']\n",
    "        has_accounting = any(indicator in filename_lower for indicator in accounting_indicators)\n",
    "\n",
    "        finance_indicators = ['fin', 'finance']\n",
    "        has_finance = any(indicator in filename_lower for indicator in finance_indicators)\n",
    "\n",
    "        # After checking all syllabus, there is no file that is both accounting and finance\n",
    "        if has_accounting:\n",
    "            return \"Accounting\", 0.9\n",
    "        elif has_finance:  # Syllabus whose filename contains finance might be an accounting syllabus\n",
    "            return \"Finance\", 0.3  # So we impose a lower confidence\n",
    "        else:\n",
    "            return \"Unknown\", 0.0\n",
    "\n",
    "    def classify(self, text, filename=None):\n",
    "        \"\"\"\n",
    "        Classify text based on keyword frequency and filename if provided\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to classify\n",
    "        filename : str, optional\n",
    "            Name of the file to classify\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (predicted_class, confidence_score)\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return \"Unknown\", 0.0\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        # Count accounting and finance keywords\n",
    "        accounting_count = sum(text.count(keyword) for keyword in self.accounting_keywords)\n",
    "        finance_count = sum(text.count(keyword) for keyword in self.finance_keywords)\n",
    "        total_count = accounting_count + finance_count\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if total_count == 0:\n",
    "            keyword_class = \"Unknown\"\n",
    "            keyword_confidence = 0.0\n",
    "        elif accounting_count >= finance_count:\n",
    "            keyword_confidence = accounting_count / (total_count or 1)  # Avoid division by zero\n",
    "            keyword_class = \"Accounting\"\n",
    "        else:\n",
    "            keyword_confidence = finance_count / (total_count or 1)  # Avoid division by zero\n",
    "            keyword_class = \"Finance\"\n",
    "\n",
    "        # If no filename provided, use only keyword-based classification\n",
    "        if filename is None:\n",
    "            return keyword_class, keyword_confidence\n",
    "\n",
    "        # Incorporate filename classification\n",
    "        filename_class, filename_confidence = self.classify_by_filename(filename)\n",
    "\n",
    "        if filename_class == keyword_class and filename_class != \"Unknown\":\n",
    "            # Both methods agree, boost confidence\n",
    "            confidence = (0.7 * keyword_confidence) + (0.3 * filename_confidence)\n",
    "            return keyword_class, min(confidence, 0.95)\n",
    "        elif filename_class != \"Unknown\":\n",
    "            # Methods disagree, but filename gives a hint\n",
    "            # Calculate combined confidence but weight keyword classification more\n",
    "            confidence = (0.8 * keyword_confidence) + (0.2 * filename_confidence)\n",
    "            return keyword_class, confidence\n",
    "        else:\n",
    "            # No useful info from filename, stick with keyword-based classification\n",
    "            return keyword_class, keyword_confidence\n",
    "\n",
    "\n",
    "class DocumentSimilarityClassifier:\n",
    "    \"\"\"Classify documents based on similarity to reference documents\"\"\"\n",
    "\n",
    "    def __init__(self, reference_accounting_doc=None, reference_finance_doc=None):\n",
    "        \"\"\"\n",
    "        Initialize with reference documents\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        reference_accounting_doc : str, optional\n",
    "            Reference accounting document text\n",
    "        reference_finance_doc : str, optional\n",
    "            Reference finance document text\n",
    "        \"\"\"\n",
    "        self.reference_accounting_doc = reference_accounting_doc\n",
    "        self.reference_finance_doc = reference_finance_doc\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "\n",
    "    def calculate_similarity(self, text, reference_doc):\n",
    "        \"\"\"\n",
    "        Calculate similarity between a document and a reference document\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to analyze\n",
    "        reference_doc : str\n",
    "            Reference document text\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float : Similarity score\n",
    "        \"\"\"\n",
    "        if not reference_doc:\n",
    "            return 0.0\n",
    "\n",
    "        # Preprocess both texts\n",
    "        preprocessed_text = self.preprocessor.preprocess(text)\n",
    "        preprocessed_reference = self.preprocessor.preprocess(reference_doc)\n",
    "\n",
    "        # Create TF-IDF vectors\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([preprocessed_text, preprocessed_reference])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def classify(self, text):\n",
    "        \"\"\"\n",
    "        Classify text based on similarity to reference documents\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to classify\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (predicted_class, confidence_score, similarity_scores)\n",
    "        \"\"\"\n",
    "        # Calculate similarity to reference documents\n",
    "        accounting_similarity = self.calculate_similarity(text, self.reference_accounting_doc)\n",
    "        finance_similarity = self.calculate_similarity(text, self.reference_finance_doc)\n",
    "\n",
    "        # Determine class based on highest similarity\n",
    "        if accounting_similarity > finance_similarity:\n",
    "            predicted_class = \"Accounting\"\n",
    "            confidence = accounting_similarity\n",
    "        else:\n",
    "            predicted_class = \"Finance\"\n",
    "            confidence = finance_similarity\n",
    "\n",
    "        # Return both the classification and the raw similarity scores\n",
    "        return predicted_class, confidence, (accounting_similarity, finance_similarity)\n",
    "\n",
    "\n",
    "class TopicModeler:\n",
    "    \"\"\"Perform topic modeling using LSI and LDA\"\"\"\n",
    "\n",
    "    def __init__(self, n_topics=2):\n",
    "        \"\"\"\n",
    "        Initialize with number of topics\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_topics : int, optional\n",
    "            Number of topics to extract\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=42,\n",
    "            max_iter=10,\n",
    "            learning_method='online'\n",
    "        )\n",
    "        self.lsi_model = None\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.85)\n",
    "\n",
    "    def fit_lda(self, preprocessed_documents):\n",
    "        \"\"\"\n",
    "        Fit LDA model to preprocessed documents\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        preprocessed_documents : dict\n",
    "            Dictionary with document names as keys and preprocessed text as values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        texts = list(preprocessed_documents.values())\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        self.lda_model.fit(self.tfidf_matrix)\n",
    "        return self\n",
    "\n",
    "    def fit_lsi(self, preprocessed_documents):\n",
    "        \"\"\"\n",
    "        Fit LSI model to preprocessed documents\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        preprocessed_documents : dict\n",
    "            Dictionary with document names as keys and preprocessed text as values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        texts = list(preprocessed_documents.values())\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "        # Create dictionary and corpus\n",
    "        self.dictionary = Dictionary(tokenized_texts)\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "        # Create LSI model\n",
    "        self.lsi_model = LsiModel(\n",
    "            self.corpus,\n",
    "            id2word=self.dictionary,\n",
    "            num_topics=self.n_topics\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_lda_topics(self, n_terms=10):\n",
    "        \"\"\"\n",
    "        Get top terms for each LDA topic\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_terms : int, optional\n",
    "            Number of top terms to return\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list : List of lists containing top terms for each topic\n",
    "        \"\"\"\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        topics = []\n",
    "\n",
    "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "            top_terms_idx = topic.argsort()[:-n_terms - 1:-1]\n",
    "            top_terms = [feature_names[i] for i in top_terms_idx]\n",
    "            topics.append(top_terms)\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_lsi_topics(self, n_terms=10):\n",
    "        \"\"\"\n",
    "        Get top terms for each LSI topic\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_terms : int, optional\n",
    "            Number of top terms to return\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list : List of lists containing top terms for each topic\n",
    "        \"\"\"\n",
    "        topics = []\n",
    "\n",
    "        for topic_id in range(self.n_topics):\n",
    "            top_terms = self.lsi_model.show_topic(topic_id, n_terms)\n",
    "            topics.append([term for term, _ in top_terms])\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def classify_with_lda(self, text, preprocess_func=None):\n",
    "        \"\"\"\n",
    "        Classify a document using LDA topic distribution\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to classify\n",
    "        preprocess_func : function, optional\n",
    "            Function to preprocess the text\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list : Topic distribution\n",
    "        \"\"\"\n",
    "        if preprocess_func:\n",
    "            text = preprocess_func(text)\n",
    "\n",
    "        # Transform text to TF-IDF\n",
    "        text_tfidf = self.vectorizer.transform([text])\n",
    "\n",
    "        # Get topic distribution\n",
    "        topic_dist = self.lda_model.transform(text_tfidf)[0]\n",
    "\n",
    "        return topic_dist.tolist()\n",
    "\n",
    "    def classify_with_lsi(self, text, preprocess_func=None):\n",
    "        \"\"\"\n",
    "        Classify a document using LSI\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            Text to classify\n",
    "        preprocess_func : function, optional\n",
    "            Function to preprocess the text\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list : Topic distribution\n",
    "        \"\"\"\n",
    "        if preprocess_func:\n",
    "            text = preprocess_func(text)\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized_text = text.split()\n",
    "\n",
    "        # Convert to bow\n",
    "        bow = self.dictionary.doc2bow(tokenized_text)\n",
    "\n",
    "        # Get topic distribution\n",
    "        topic_dist = self.lsi_model[bow]\n",
    "\n",
    "        # Convert to dense representation\n",
    "        dense_vec = np.zeros(self.n_topics)\n",
    "        for topic_id, weight in topic_dist:\n",
    "            dense_vec[topic_id] = weight\n",
    "\n",
    "        return dense_vec.tolist()\n",
    "\n",
    "\n",
    "class SyllabiAnalyzer:\n",
    "    \"\"\"Main class to analyze syllabi documents\"\"\"\n",
    "\n",
    "    def __init__(self, directory_path):\n",
    "        \"\"\"\n",
    "        Initialize with directory containing PDF files\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        directory_path : str\n",
    "            Path to directory containing PDF files\n",
    "        \"\"\"\n",
    "        self.directory_path = directory_path\n",
    "        self.pdf_processor = PDFProcessor(directory_path)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        self.clustering = DocumentClustering()\n",
    "        self.raw_documents = {}\n",
    "        self.preprocessed_documents = {}\n",
    "        self.similarity_classifier = None\n",
    "        self.keyword_classifier = KeywordClassifier()\n",
    "        self.topic_modeler = TopicModeler()\n",
    "        self.reference_accounting_doc = None\n",
    "        self.reference_finance_doc = None\n",
    "        self.document_similarity_classifier = None\n",
    "\n",
    "    def process_documents(self):\n",
    "        \"\"\"Process all documents in the directory\"\"\"\n",
    "        self.raw_documents = self.pdf_processor.extract_all_texts()\n",
    "        self.preprocessed_documents = self.text_preprocessor.preprocess_documents(self.raw_documents)\n",
    "        return self\n",
    "\n",
    "    def set_reference_documents(self, accounting_filename, finance_filename):\n",
    "        \"\"\"\n",
    "        Set reference documents for similarity comparison\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        accounting_filename : str\n",
    "            Filename of the reference accounting document\n",
    "        finance_filename : str\n",
    "            Filename of the reference finance document\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        accounting_path = os.path.join(self.directory_path, accounting_filename)\n",
    "        finance_path = os.path.join(self.directory_path, finance_filename)\n",
    "\n",
    "        self.reference_accounting_doc = PDFProcessor.extract_text(accounting_path)\n",
    "        self.reference_finance_doc = PDFProcessor.extract_text(finance_path)\n",
    "\n",
    "        # Initialize the similarity classifier with reference documents\n",
    "        self.document_similarity_classifier = DocumentSimilarityClassifier(\n",
    "            self.reference_accounting_doc,\n",
    "            self.reference_finance_doc\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def perform_clustering(self):\n",
    "        \"\"\"Perform document clustering\"\"\"\n",
    "        self.clustering.fit(self.preprocessed_documents)\n",
    "\n",
    "        # Initialize the similarity classifier with the clustering model\n",
    "        self.similarity_classifier = SimilarityClassifier(\n",
    "            self.clustering.vectorizer,\n",
    "            self.clustering.kmeans\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def analyze_clusters(self):\n",
    "        \"\"\"Analyze the formed clusters\"\"\"\n",
    "        # Get top terms for each cluster\n",
    "        cluster_terms = self.clustering.get_cluster_terms()\n",
    "\n",
    "        # Get documents in each cluster\n",
    "        cluster_docs = self.clustering.get_cluster_documents()\n",
    "\n",
    "        # Identify potential outliers\n",
    "        outliers = self.clustering.identify_outliers()\n",
    "\n",
    "        # Get representative documents for each cluster\n",
    "        representative_docs = self.clustering.get_representative_documents()\n",
    "\n",
    "        # Map clusters to subjects based on terms\n",
    "        cluster_mapping = {}\n",
    "        for cluster_id, terms in cluster_terms.items():\n",
    "            if any(term in ['financial statement', 'balance sheet', 'accounting'] for term in terms):\n",
    "                cluster_mapping[cluster_id] = \"Accounting\"\n",
    "            else:\n",
    "                cluster_mapping[cluster_id] = \"Finance\"\n",
    "\n",
    "        # Update similarity classifier with mapping\n",
    "        self.similarity_classifier.cluster_mapping = cluster_mapping\n",
    "\n",
    "        # If reference documents are not set, use representative documents\n",
    "        if self.reference_accounting_doc is None or self.reference_finance_doc is None:\n",
    "            accounting_cluster = next((k for k, v in cluster_mapping.items() if v == \"Accounting\"), None)\n",
    "            finance_cluster = next((k for k, v in cluster_mapping.items() if v == \"Finance\"), None)\n",
    "\n",
    "            if accounting_cluster is not None and finance_cluster is not None:\n",
    "                accounting_doc = representative_docs.get(accounting_cluster)\n",
    "                finance_doc = representative_docs.get(finance_cluster)\n",
    "\n",
    "                if accounting_doc and finance_doc:\n",
    "                    accounting_path = os.path.join(self.directory_path, accounting_doc)\n",
    "                    finance_path = os.path.join(self.directory_path, finance_doc)\n",
    "\n",
    "                    self.reference_accounting_doc = PDFProcessor.extract_text(accounting_path)\n",
    "                    self.reference_finance_doc = PDFProcessor.extract_text(finance_path)\n",
    "\n",
    "                    # Initialize the similarity classifier with reference documents\n",
    "                    self.document_similarity_classifier = DocumentSimilarityClassifier(\n",
    "                        self.reference_accounting_doc,\n",
    "                        self.reference_finance_doc\n",
    "                    )\n",
    "\n",
    "        return cluster_mapping, outliers, cluster_terms, cluster_docs\n",
    "\n",
    "    def perform_topic_modeling(self):\n",
    "        \"\"\"Perform topic modeling using LDA and LSI\"\"\"\n",
    "        # Fit LDA model\n",
    "        self.topic_modeler.fit_lda(self.preprocessed_documents)\n",
    "\n",
    "        # Fit LSI model\n",
    "        self.topic_modeler.fit_lsi(self.preprocessed_documents)\n",
    "\n",
    "        # Get topics from LDA\n",
    "        lda_topics = self.topic_modeler.get_lda_topics()\n",
    "\n",
    "        # Get topics from LSI\n",
    "        lsi_topics = self.topic_modeler.get_lsi_topics()\n",
    "\n",
    "        return lda_topics, lsi_topics\n",
    "\n",
    "    def evaluate_classification_methods(self, test_documents=None):\n",
    "        \"\"\"\n",
    "        Evaluate different classification methods\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_documents : dict, optional\n",
    "            Dictionary with document names as keys and raw text as values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Evaluation results\n",
    "        \"\"\"\n",
    "        # If no test documents provided, use a subset of existing documents\n",
    "        if test_documents is None:\n",
    "            test_docs = {}\n",
    "            for i, (name, text) in enumerate(self.raw_documents.items()):\n",
    "                if i % 5 == 0:  # Use every 5th document as test\n",
    "                    test_docs[name] = text\n",
    "\n",
    "            test_documents = test_docs\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for doc_name, raw_text in test_documents.items():\n",
    "            preprocessed_text = self.text_preprocessor.preprocess(raw_text)\n",
    "\n",
    "            # Check if it's a syllabus\n",
    "            is_syllabus = self.keyword_classifier.is_syllabus(raw_text)\n",
    "            if not is_syllabus:\n",
    "                results[doc_name] = {\n",
    "                    \"is_syllabus\": False,\n",
    "                    \"similarity\": (\"Not a Syllabus\", 0.0, True),\n",
    "                    \"keyword\": (\"Not a Syllabus\", 0.0),\n",
    "                    \"document_similarity\": (\"Not a Syllabus\", 0.0, (0.0, 0.0)),\n",
    "                    \"lda\": (\"Not a Syllabus\", 0.0),\n",
    "                    \"lsi\": (\"Not a Syllabus\", 0.0)\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # Similarity-based classification\n",
    "            sim_class, sim_conf, is_outlier = self.similarity_classifier.classify(\n",
    "                raw_text,\n",
    "                preprocess_func=self.text_preprocessor.preprocess\n",
    "            )\n",
    "\n",
    "            # Keyword-based classification\n",
    "            kw_class, kw_conf = self.keyword_classifier.classify(raw_text, doc_name)\n",
    "\n",
    "            # Document similarity classification\n",
    "            if self.document_similarity_classifier:\n",
    "                doc_sim_class, doc_sim_conf, similarities = self.document_similarity_classifier.classify(raw_text)\n",
    "            else:\n",
    "                doc_sim_class, doc_sim_conf, similarities = \"Unknown\", 0.0, (0.0, 0.0)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiningxiang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1b5123eaa8b3f27d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
